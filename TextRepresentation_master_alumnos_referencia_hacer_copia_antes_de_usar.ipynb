{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "TextRepresentation_master_alumnos_referencia_hacer_copia_antes_de_usar.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Borja-Hoyos/Text_Minning/blob/main/TextRepresentation_master_alumnos_referencia_hacer_copia_antes_de_usar.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A48-kUHmOmQZ"
      },
      "source": [
        "# Representación numérica de documentos\n",
        "\n",
        "En este notebook veremos como representar documentos siguiendo la metodología Bag of Words (BoW). \n",
        "\n",
        "La metodología Bag of Words (BoW) representa vectorialmente documentos ignorando el orden de las palabras. Para implementar este método hay que disponer de tres elementos:\n",
        "\n",
        " 1. Un corpus con varios documentos\n",
        " 2. Un vocabulario de las palabras de nuestro corpus\n",
        " 3. Una métrica para medir la presencia o no de palabras en los documentos.\n",
        "\n",
        "\n",
        "En esta ocasión utilizaremos las funcionalidades que nos brinda Scikit-Learn para calcular la representación de nuestros textos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvFyVzubOmJM"
      },
      "source": [
        "## Obtención de un corpus\n",
        "\n",
        "Para este ejercicio vamos a trabajar con un corpus pequeño, que nos permita ver y entender fácilmente los parámetros de las funciones."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3lP4QBh6TBT3"
      },
      "source": [
        "corpus = [\"Yo quiero agua\",\n",
        "          \"Yo quiero cocacola\",\n",
        "          \"Yo quiero agua y un agua\",\n",
        "          \"Yo no quiero vino\",\n",
        "          \"Yo quiero un entrecot\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eok7CtkJTKVW"
      },
      "source": [
        "## Vocabulario de palabras\n",
        "\n",
        "En scikit-learn podemos utilizar distintas funciones para obtener el vocabulario de un corpus de documentos. Ambas están presentes dentro del módulo feature_extraction.text y son [`CountVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) y  [`TfidfVectorizer`](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html#sklearn.feature_extraction.text.TfidfVectorizer). \n",
        "Vamos a crear un objeto con cada una de esas clases para introducir nuestro corpus y extraer el vocabulario."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TJl8IO50AKBp"
      },
      "source": [
        "\n",
        "**tTanto las funciones CountVectorizer como TfidfVectorizer cuentan con muchos parámetros personalizables. Algunos de los más relevantes son:**\n",
        "\n",
        "\n",
        "*   *strip_accents*: Elimina los acentos en codificación ascii o unicode. Por defecto es None. Es preferible hacer una gestión de acentos previas.\n",
        "*   *lowercase*: Transforma todos los caracteres a minúsculas antes de hacer la tokenización.\n",
        "*   *tokenizer*: Utiliza un tokenizador específico. Se puede utilizar una de NLTK o de Spacy(computacionalmente menos eficiente).\n",
        "*   *stop_words*: Si se pone el valor \"english\" eliminara la lista de stop_words definida en scikit-learn. Se puede utilizar la lista de stopwords de otras librerías o definir unas propias.\n",
        "*   *ngram_range*: Cálculo de n-gramas en el proceso. Mediante la tubla (min_n, max_n) se pueden incorporar n-gramas al cálculo de la matriz tfidf.\n",
        "*   *max_df*: Valor por defecto 1.  Ignora los tokens (o n-gramas) que aparecen en más del X % de documentos cuando es menor de 1. Si max_df es mayor que uno se ignorarán los términos que aparecen en más de X documentos.\n",
        "*   *min_df*: Valor por defecto 1. Ignora los tokens que aparecen  en menos del X % de los documentos. Siendo X el valor de X. (0.01 = 1%, por ejemplo)\n",
        "*   *max_features*: Máximas características que devuelve la función TfidfVectorizer. Valor mayor que 1. Representa las caracaterísticas más importantes (las más repetidas o comunes) Esto es muy interesante para no sobreentrenar el sistema.\n",
        "*   *norm*: Valores \"l1\" y \"l2\", por defecto \"l2\". Normaliza los valores entre 0 y 1.\n",
        "*   *use_idf*: Habilita el uso del inverse-document frequency en la función. Por defecto es True.\n",
        "*   *smooth_idf*: Suaviza los pesos de IDF sumando una unidad a cada frecuencia. Es muy importante para evitar divisiones por cero.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PiDKf5zHT-Rm"
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\n",
        "from sklearn.feature_extraction import text\n",
        "\n",
        "# Creamos los objetos\n",
        "count_vectorizer = CountVectorizer(___________________)\n",
        "tfidf_vectorizer = TfidfVectorizer(___________________) # Re-escribimos los valores por defecto para tener el tf-idf básico\n",
        "\n",
        "# Hacemos Fit con nuestro corpus\n",
        "count_data = ________________________\n",
        "tfidf_data = _________________________\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMivvvOBtmdf",
        "outputId": "fb083684-badd-4f86-8bf4-175890ad575b"
      },
      "source": [
        "count_vectorizer"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
              "                lowercase=True, max_df=1.0, max_features=7, min_df=1,\n",
              "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
              "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4tUVaA_-UItk"
      },
      "source": [
        "# Obtenemos los vocabularios de dos formas:\n",
        "print(\"COUNT VECTORIZER\")\n",
        "print(\"Obtenemos un dictionario que mapea las palabras con su posición en el vector del vocabulario\")\n",
        "print(count_data.vocabulary_)\n",
        "print(\"Obtenemos el vocabulario en si mismo como una lista\")\n",
        "print(count_data.get_feature_names())\n",
        "print(\"Numero de características:\")\n",
        "print(len(count_data.get_feature_names()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1Xi6tt1DH-0"
      },
      "source": [
        "# Obviamente, dado que hemos utilizado el mismo corpus, obtenemos el mismo resultado con el tfidf_vectorizer.\n",
        "print(\"\\n\\n TF-IDF VECTORIZER\")\n",
        "print(\"Obtenemos un dictionario que mapea las palabras con su posición en el vector del vocabulario\")\n",
        "print(tfidf_vectorizer._______________________)\n",
        "print(\"Obtenemos el vocabulario en si mismo como una lista\")\n",
        "print(tfidf_vectorizer._________________________)\n",
        "print(\"Numero de características:\")\n",
        "print(len(tfidf_vectorizer.______________________))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQabDt3aDJ4-"
      },
      "source": [
        "Si imprimimos el objeto tfidf_data podemos ver la configuración de los parámetros de TfidfVectorizer:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UhHyIi8JUiLn",
        "outputId": "70e4c41d-e035-4969-8871-ebacc1409b4c"
      },
      "source": [
        "tfidf_data"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
              "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
              "                input='content', lowercase=True, max_df=1.0, max_features=None,\n",
              "                min_df=1, ngram_range=(1, 1), norm=None, preprocessor=None,\n",
              "                smooth_idf=False, stop_words=None, strip_accents=None,\n",
              "                sublinear_tf=False, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
              "                tokenizer=None, use_idf=True, vocabulary=None)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lBUeudl0Ol8L"
      },
      "source": [
        "## Resultados con distintas métricas\n",
        "En esta ocasión vamos a mostrar los resultados con dos métricas distintas. El `CountVectorizer` mostrará el conteo de veces que una palabra del vocabulario está presente dentro del documento, el `TfidfVectorizer` mostrará el resultado con la métrica TF-IDF mostrada en los apuntes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y7BGfa8UAEvE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9bd7130f-0042-4270-f5d2-61db5e90174e"
      },
      "source": [
        "# Resultado del CountVectorizer\n",
        "count_data_result = count_data.transform(corpus).toarray()\n",
        "print(count_data_result)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[[1 0 0 0 1 0 1]\n",
            " [0 1 0 0 1 0 1]\n",
            " [2 0 0 0 1 1 1]\n",
            " [0 0 0 1 1 0 1]\n",
            " [0 0 1 0 1 1 1]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fnvyxz32DdUG"
      },
      "source": [
        "# Resultado del TfidfVectorizer\n",
        "tfidf_data_result = tfidf_data.________________________.toarray()\n",
        "print(tfidf_data_result)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_xm5fMDoY-nR"
      },
      "source": [
        "Vamos a mostrar los resultados con seaborn para que se vean mejor. Importante mencionar que esto se puede hacer cuando el vocabulario es muy reducido, si no podría ocasionar problemas en la memoria."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cck6bfUBZBrb"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "plt.figure(figsize=(16, 6))\n",
        "# Figura CountVectorizer\n",
        "sns.heatmap(count_data_result, annot=True,cbar=False,\n",
        "            xticklabels=_____________________________,\n",
        "            yticklabels = [\"Frase 1\", \"Frase 2\", \"Frase 3\",\"Frase 4\", \"Frase 5\"])\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X4viXEKmD1FT"
      },
      "source": [
        "plt.figure(figsize=(16, 6))\n",
        "sns.heatmap(tfidf_data_result, annot=True,cbar=False,\n",
        "            xticklabels=_______________________________________,\n",
        "            yticklabels = [\"Frase 1\", \"Frase 2\", \"Frase 3\",\"Frase 4\", \"Frase 5\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JFJMRxoQNK4J"
      },
      "source": [
        "# ¿Cómo se utilizaría otro tokenizer u otra lista de palabras vacías?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vov1rGLIDjr9"
      },
      "source": [
        "En primer lugar, dado que vamos a utilizar spacy, instalaremos la librería y el modelo:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZshEoLThWJDt"
      },
      "source": [
        "!pip install spacy\n",
        "!python -m spacy download es_core_news_sm"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aNiPnKMIDqVA"
      },
      "source": [
        "Importamos las librerías y creamos el objeto \"nlp\" para procesar los textos\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FQLM1Ja1Wtak"
      },
      "source": [
        "import spacy\n",
        "import es_core_news_sm\n",
        "nlp = es_core_news_sm.load()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2YKq_qpdDxHA"
      },
      "source": [
        "Guardamos las stop words de Spacy en una variable llamada \"stop_words\". También cogemos los tokens considerados símbolos de punctuación en la variable \"punctuations\":"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x_Rnj35hXCH_"
      },
      "source": [
        "import string\n",
        "spacy_stopwords = spacy.lang.es.stop_words.STOP_WORDS\n",
        "stop_words = spacy_stopwords\n",
        "punctuations=string.punctuation"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NFUlkdsEBUu"
      },
      "source": [
        "Generamos una función \"spacy_tokenizer\" que:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k6pGl_YBO7uZ"
      },
      "source": [
        "def spacy_tokenizer(sentence):\n",
        "    # Pasamos la frase por el objeto nlp para procesarla\n",
        "    mytokens = nlp(sentence)\n",
        "\n",
        "    # Lematizamos los tokens y los convertimos  a minusculas\n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"--PRON\" else word.lower_ for word in mytokens ]\n",
        "\n",
        "    # Quitamos las stopwords y los signos de puntuacion\n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "\n",
        "    # devolver una lsita de tokens\n",
        "    return mytokens"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zzZbGtGaD-Vt"
      },
      "source": [
        "Utilizamos esa función como tokenizador en TfidfVectorizer:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "niPHblSRO7jM"
      },
      "source": [
        "tfidf_vectorizer = TfidfVectorizer(norm=None, smooth_idf=False, tokenizer = ________________________) # Re-escribimos los valores por defecto para tener el tf-idf básico\n",
        "tfidf_data = tfidf_vectorizer.fit(corpus)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BjKhD6FJEY7G"
      },
      "source": [
        "print(\"\\n\\n TF-IDF VECTORIZER\")\n",
        "print(\"Obtenemos un dictionario que mapea las palabras con su posición en el vector del vocabulario\")\n",
        "print(tfidf_vectorizer._________________)\n",
        "print(\"Obtenemos el vocabulario en si mismo como una lista\")\n",
        "print(tfidf_vectorizer.________________)\n",
        "print(\"Numero de características:\")\n",
        "print(len(tfidf_vectorizer.__________________))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70q1LsT6BnQ_"
      },
      "source": [
        "**Ejercicio**: \n",
        "\n",
        "Queremos transformar nuestro dataset de noticias con las siguientes especificaciones:\n",
        " - Transformar los primeros 3000 documentos \n",
        " - Se utilice idf y el valor de norm por defecto.\n",
        " - Se utilice la función de preprocesado de spacy utilizada anteriormente\n",
        " - Se consideren unigramas, bigramas, trigramas\n",
        " - Se consideren un máximo de 250 características (max_features)\n",
        " - El vectorizador no debe considerar los elementos que aparezcan en menos del 5% de documentos. (min_df)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdgVDoaZBmdI"
      },
      "source": [
        "# Dataset de noticias\n",
        "!wget \"https://github.com/luisgasco/ntic_master_datos/raw/main/datasets/news_summary.csv\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e0AlbRlGCzBp"
      },
      "source": [
        "import pandas as pd\n",
        "news_summary = pd.read_csv('../content/news_summary.csv', encoding='latin-1')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tUTG5XNL5vOz"
      },
      "source": [
        "subset = news_summary[\"text\"].to_list()[0:3000]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uc-mjG6V55CS"
      },
      "source": [
        "tfidf_vect = TfidfVectorizer(_____________________________________________)\n",
        "tfidf_data = tfidf_vect.fit(______________________________________)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BsLoCWue65R3"
      },
      "source": [
        "tfidf_data.get_feature_names()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vCd_8lZM58Di"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}